# 第一节 RLHF 技术详解

## 一、模型对齐的必要性

通过前面的学习，我们已经知道如何通过 PEFT，用较低的成本让大语言模型适应下游任务。无论是 PEFT 还是全量微调，主要方法大多是**有监督微调（Supervised Fine-tuning, SFT）**，即用成对的“(指令, 回答)”数据来训练模型。但是，SFT 存在一个重要的局限。它教会模型**模仿**高质量的范例，但无法让模型理解**人类的偏好**。一个 SFT 模型或许能很好地回答“中国的首都是哪里？”，但当面对更开放、更复杂的指令，如“为我的项目写一个既专业又有创意的介绍”时，SFT 的局限性就暴露无遗：

- **缺乏泛化能力**：模型只能很好地响应数据集中出现过的指令模式，无法覆盖用户千奇百怪的真实意图。
- **“对齐”不足**：模型的回答可能在事实上正确，但在风格、语气、安全性或有用性上，并不符合人类的期望。它只知道“正确答案”，却不知道“更好的答案”。

为了跨越从“遵循指令”到“理解偏好”的鸿沟，我们需要一种新的训练范式。**基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）** 正是解决这一问题的关键技术。它的核心目标就是让模型超越简单的模仿学习，真正理解并内化人类复杂的价值观，使其输出更符合我们的期望。

如图 12-1 所示，在 InstructGPT 的研究中[^1]，经过 RLHF 对齐后，仅有 13 亿参数的模型，在人类评估中的表现甚至超过了 1750 亿参数的原始 GPT-3 模型。

<p align="center">
  <img src="./images/12_1_1.png" width="70%" alt="InstructGPT 模型与 GPT-3 的人类偏好对比" />
  <br />
  <em>图 12-1：InstructGPT 与 GPT-3 人类偏好对比</em>
</p>

> 需要明确的是，PEFT 和 RLHF 是两条解决不同问题的技术路线，它们**正交且互补**：
> 
> - **PEFT** 是一套**方法**，旨在解决**效率问题**，即如何用更少的资源来微调模型。
> - **RLHF** 则是一个**范式**，旨在解决**对齐问题**，即如何让模型的输出更符合人类偏好。

## 二、通往对齐模型的三大阶段

在实践中，对齐训练通常划分为三个阶段，分别面向通用语言能力、指令遵循与偏好对齐：

### 2.1 基础模型预训练

在大规模（万亿级 Token）无标注文本数据（网页、书籍、代码等）上进行自回归预训练。

-   **目标**：预测下一个词。
-   **结果**：得到基础语言模型，具备广泛的语料知识和生成能力，但未进行指令/偏好对齐。
-   **成本与难点**：训练成本高，且需在超大规模数据与模型上保证稳定收敛。

### 2.2 有监督指令微调

使用少量（千到几万条）高质量“指令-回答”对，对基础模型进行有监督微调，使其学会理解并执行人类指令。

**目标**：在指令上下文中进行条件生成，提升指令遵循与任务完成能力。  
**数据**：常见格式为 `(prompt, response)` 对，可结合 QLoRA 等参数高效化技术构造与训练。  
**输出**：获得“指令模型”（Instruction-tuned LM），在指定任务与约束条件下更可控。

#### 2.2.1 SFT 数据集分类

高质量的 SFT 数据集是模型能力养成的关键。根据目标的不同，数据集可以分为两类：

1.  **任务型指令数据集**：如 `WizardLM Evol‑Instruct 70k` (AI 生成) 和 `Dolly-15k` (专家编写)，它们主要由单轮的“指令-回答”构成，核心目标是教会模型如何“做事”，准确地遵循指令完成特定任务。

2.  **对话型数据集**：这类数据集的代表作就是 **OpenAssistant (OASST)**。它专注于教会模型如何“聊天”，其独特性和重要性体现在：
    *   **来源真实且多样**：数据由超过 1.3 万名真实志愿者众包贡献，覆盖了广泛的话题和真实的语言风格，而非 AI 生成或专家编写的固定模式。
    *   **专注于多轮对话**：OASST 的核心是**多轮对话树**，完整记录上下文、分支和用户的追问。这对于训练模型理解上下文、进行连贯的长对话至关重要。
    *   **丰富的反馈信号**：数据不仅包含对话文本，还有人工标注的质量评分和多种标签（可作为偏好学习/奖励建模的有益信号）。这使其不仅是优秀的 SFT 数据，也是训练奖励模型（Reward Model）、进行 RLHF 的宝贵材料。

简单来说，任务型数据集教会模型“智商”，而以 OASST 为代表的对话型数据集则赋予模型“情商”，使其更接近一个真正能与人交流的智能助手。

### 2.3 基于人类反馈的强化学习

该阶段基于人类偏好信号进一步优化策略，不再依赖固定的“标准答案”，而是让模型学习“更被偏好”的行为模式。

1. **目标**：在“有用性（helpfulness）”“无害性（harmlessness）”“诚实性（honesty）”等维度实现与人类偏好的对齐。  
2. **方法**：采用强化学习框架，以奖励模型提供的标量奖励或等价的偏好约束为信号，优化语言模型策略。

## 三、RLHF 的核心步骤

如图 12-2 所示，RLHF 的流程主要包含三个核心步骤。首先通过有监督微调得到初始策略模型；然后，收集人类偏好数据训练一个奖励模型；最后，使用奖励模型作为信号，通过强化学习算法（如 PPO）进一步优化策略模型。

<p align="center">
  <img src="./images/12_1_2.png" width="80%" alt="RLHF 经典三步法示意图" />
  <br />
  <em>图 12-2：RLHF 经典三步法示意图</em>
</p>

### 3.1 训练奖励模型 (Training a Reward Model, RM)

在 RLHF 中，我们需要一个“裁判”，用来评判模型的哪个回答更好；这个“裁判”就是奖励模型（RM）。RM 的目标是学习一个函数 $r(x, y)$，输入提示 $x$ 和回答 $y$，输出一个标量分数，代表人类对该回答的偏好程度。在 InstructGPT 的实践中，RM 数据集约含 33k 个训练提示（由此衍生出数量级更大的成对比较样本）。接下来，分别介绍一下如何收集人类偏好数据，以及训练奖励模型。

#### 3.1.1 收集人类偏好数据

第一步需要准备一个指令（prompt），可从真实用户请求中选取，也可由标注员设计；让 SFT 模型针对该指令生成多个（InstructGPT 中为 4-9 个）不同的回答；由人工标注员对这些回答进行**排序（Ranking）**，从最好到最差。相比于给每个回答打一个绝对分数，排序是一种对人类更友好、也更一致的标注方式。这些排序数据构成了偏好数据集 $\mathcal{D} = \{(x, y_w, y_l)\}_{i=1}^N$，其中 $y_w$ 是比 $y_l$ 更受偏好的回答 (winner)，$y_l$ 是较差的回答 (loser)。

#### 3.1.2 训练奖励模型

训练奖励模型的常见做法包括以下几点。

-   将一个包含 K 个回答的排序数据，转换成 $\binom{K}{2}$ 个成对的比较数据。例如，(回答A > 回答B)、(回答A > 回答C)、(回答B > 回答C)...

-   奖励模型（RM）通常和我们正在优化的语言模型结构类似（但可以小得多），它的任务不是生成文本，而是**输入 `(prompt, response)`，输出一个标量分数 `reward`**。

-   训练 RM 的目标是，让它给人类偏好的回答打出更高的分数。这通常基于 **Bradley-Terry 模型**，该模型假设人类对两个回答的偏好概率可以用它們的潜在奖励分数通过 Sigmoid 函数来建模：

    $$
    P(y_w \succ y_l | x) = \sigma(r(x, y_w) - r(x, y_l))
    $$
    
    其损失函数（负对数似然）如下：

    $$
    \text{loss}(\theta) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log(\sigma(r_\theta(x, y_w) - r_\theta(x, y_l))) \right]
    $$

    其中 $r_\theta$ 是奖励模型，$y_w$ 是比 $y_l$ 更受偏好的回答，$\sigma$ 是 Sigmoid 函数。这个损失函数旨在最大化偏好回答与非偏好回答之间的分数差距。

经过训练，RM 能够逼近人类偏好函数，作为自动化的偏好评估器，为后续策略优化提供稳定的标量奖励信号。

### 3.2 使用策略优化算法微调模型

有了“裁判”（或者隐式的偏好信号），就可以开始真正的“强化学习”训练了。这个阶段的目标是更新语言模型的**策略**，使其生成的回答能够获得更高的奖励。

传统的强化学习算法存在高方差、训练不稳定的问题。因此，学术界和工业界发展出了一系列更先进的策略优化算法，其中 PPO 和 DPO 是目前的主流。

#### 3.2.1 PPO 与“对齐税”

**近端策略优化（Proximal Policy Optimization, PPO）** 是 RLHF 中最经典的算法[^2]。PPO 的主要思路是，在尝试最大化奖励的同时，通过一个约束项来限制新旧策略的差异范围，继而避免单步更新过大导致训练崩溃。它通过优化一个“替代目标函数”（Surrogate Objective）来实现这一点。PPO 最常用的替代目标是 **Clipped Surrogate Objective**:

$$
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right) \right]
$$

其中，$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ 是新旧策略的概率比，$\hat{A}_t$ 是优势函数估计，$\epsilon$ 是一个限制更新范围的超参数（通常为 0.2）。这个裁剪操作有效地构建了一个悲观的下界，防止策略更新过于激进。

如图 12-3 是 PPO 裁剪代理目标函数 $L^{CLIP}$ 的单步示意图。当优势 $\hat{A}_t>0$ 时（左），目标函数随概率比 $r_t(\theta)$ 的增加而增加，但增长被限制在 $1+\epsilon$ 处；当 $\hat{A}_t<0$ 时（右），目标函数随 $r_t(\theta)$ 的增加而减小，但减小幅度被限制在 $1-\epsilon$ 处，从而约束了策略更新的步长。

<p align="center">
  <img src="./images/12_1_3.png" width="80%" alt="PPO 的裁剪代理目标函数" />
  <br />
  <em>图 12-3：PPO 的裁剪代理目标函数</em>
</p>

但研究者发现，单纯用 PPO 优化奖励模型，可能会导致模型在某些传统 NLP 任务（如 SQuAD）上的性能下降，这种现象被称为 **“对齐税”（Alignment Tax）**。

为了解决这个问题，InstructGPT 提出了一种名为 **PPO-ptx** 的变体，它在优化奖励的同时，混合了一部分预训练数据的梯度，其优化目标为：

$$
\text{objective}(\phi) = \mathbb{E}_{(x, y) \sim D_{\pi_\phi^{RL}}} [r_\theta(x, y) - \beta \log(\pi_\phi^{RL}(y|x) / \pi^{SFT}(y|x))] + \gamma \mathbb{E}_{x \sim D_{pretrain}}[\sum_t \log(\pi_\phi^{RL}(x_t|x_{<t}))]
$$

其中：
-   PPO 主目标项（奖励 + KL 约束）：在当前策略生成的数据分布 $D_{\pi_\phi^{RL}}$ 上，最大化奖励模型分数 $r_\theta(x,y)$，并用每 token KL 约束惩罚新策略 $\pi_\phi^{RL}$ 偏离参考模型 $\pi^{SFT}$，以抑制灾难性漂移。

-   预训练保留项（ptx）：在预训练语料 $D_{pretrain}$ 上进行最大似然学习 $\sum_t \log \pi_\phi^{RL}(x_t|x_{<t})$，用于保留通用语言能力与知识面，缓解“对齐税”。

-   符号说明：$\beta$ 控制 KL 约束强度；$\gamma$ 控制 ptx 项权重；$D_{\pi_\phi^{RL}}$ 为在线由当前策略采样的数据分布；$D_{pretrain}$ 为静态预训练语料；KL 为每 token KL 散度。

通过图 12-4 可以看到 PPO-ptx 缓解了 RLHF 在部分公开 NLP 数据集上的性能下降（对齐税）问题。图中 PPO-ptx 模型（红色）代表混合了预训练梯度，而 PPO 模型（橙色）则没有。在 SQuADv2、DROP 等多个任务上，PPO-ptx 的性能显著优于单纯的 PPO。

<p align="center">
  <img src="./images/12_1_4.png" width="90%" alt="PPO-ptx 缓解对齐税" />
  <br />
  <em>图 12-4：PPO-ptx 缓解对齐税</em>
</p>

#### 3.2.2 直接偏好优化（DPO）

尽管 PPO 效果很好，但它流程复杂（需要训练奖励模型、价值模型，并进行在线采样），训练成本高且不稳定。**直接偏好优化（Direct Preference Optimization, DPO）** 是一项创新的技术[^3]，它巧妙地绕过了显式的奖励模型训练和复杂的强化学习过程。

-   **主要思路**：DPO 证明了，我们可以从人类偏好数据 $\mathcal{D} = \{(x, y_w, y_l)\}$ 中，直接推导出最优策略，而无需先拟合一个奖励模型。它将问题转化为一个简单的分类任务，**不需要显式训练奖励模型，也无需在线采样**，所以更稳定、高效。

-   **优化目标**：
    DPO 的损失函数直接最大化模型对“更好”回答的偏好概率，同时用 KL 散度进行约束：

    $$
    \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]
    $$

    其中 $\pi_{\text{ref}}$ 是参考模型（通常是 SFT 模型），$\sigma$ 是 Sigmoid 函数。这个损失函数简洁地鼓励模型提高 $y_w$ 的概率，降低 $y_l$ 的概率。

-   **训练动力学**：DPO 的梯度更新有一个直观的解释。它会给那些**被隐式奖励模型错误排序的样本（即认为 $y_l$ 比 $y_w$ 更好）更大的权重**，从而集中火力修正模型的“判断失误”。这个隐式奖励由下式定义：
    $$
    \hat r_\theta(x,y)=\beta \log \frac{\pi_\theta(y|x)}{\pi_{\mathrm{ref}}(y|x)}
    $$

-   **优势**：由于其简单、稳定且高效，DPO 及其变种正在迅速成为 RLHF 的新范式。

如图 12-5，该图展示了在 IMDb 情感生成任务中，不同偏好学习算法的 Reward-KL 效率前沿。DPO（黄色散点）在所有 KL 散度值上都获得了最高的期望奖励，表明 DPO 能够更有效地在最大化奖励和与参考模型的 KL 散度约束之间进行权衡，其优化效果优于 PPO 等基线方法。

<p align="center">
  <img src="./images/12_1_5.png" width="80%" alt="DPO 与 PPO 的 Reward-KL 效率前沿对比" />
  <br />
  <em>图 12-5：DPO 与 PPO 的 Reward-KL 效率前沿对比</em>
</p>

#### 3.2.3 PPO 与 DPO 的选择

- **DPO**: 当你拥有一个固定的离线偏好数据集，并且追求**训练效率和稳定性**时，DPO 是首选。它实现简单，无需复杂的 RL 流程和在线采样，非常适合资源有限或希望快速迭代的场景，如对话、摘要生成。

- **PPO**: 如果你需要**更精细地控制模型的行为**，或者你的系统已经有了成熟的 RL 训练框架时，PPO 仍然是一个强大的选项。它通过显式的奖励模型和价值函数，可以进行更灵活的在线探索和策略优化，尤其是在需要模型与环境进行多步交互的复杂任务中。

### 3.3 RLHF 的实际效果

以 InstructGPT 为例，RLHF 带来了显著且复杂的影响：

<p align="center">
  <img src="./images/12_1_6.png" width="70%" alt="InstructGPT 在 TruthfulQA 上的真实性表现" />
  <br />
  <em>图 12-6：InstructGPT 在 TruthfulQA 上的真实性表现</em>
</p>

如图 12-6 展示了模型在 TruthfulQA 基准上的真实性表现。其中，灰色柱表示“真实性（truthfulness）”，彩色柱表示“真实性与信息量（truthfulness and informativeness）”。整体来看，经过 RLHF 的 InstructGPT 模型（PPO-ptx/PPO）在 TruthfulQA 上相较 GPT-3 基线更真实且更具信息量。但需注意，原论文报告 1.3B 的 PPO-ptx 模型在 TruthfulQA 上略低于同尺寸 GPT-3。

更具体地说，这些效果体现在以下几个方面：

-   **提升真实性**：在 TruthfulQA 等基准上，模型生成真实、信息丰富答案的频率提升了约一倍，且在封闭问答中“捏造事实”的比例减半。

-   **降低有害性**：当被明确指示要“尊重地”回答时，模型产生有毒输出的比例比原始 GPT-3 减少了约 25%。

-   **偏见问题依然存在**：在衡量社会偏见的数据集（如 Winogender）上，RLHF 并没有带来明显改善。

-   **仍会犯错**：模型有时仍会盲目遵循错误的指令前提（例如，回答“为什么要饭后吃袜子”），或者在简单问题上过度“耍滑头”、含糊其辞。

### 3.4 RLHF 的实践挑战与前沿方向

尽管 RLHF 效果显著，但它也面临着巨大的挑战，许多问题仍在探索之中。一篇近期的综述论文[^4]系统性地梳理了 RLHF 在文化、多模态和效率等方面的各项挑战：

-   **奖励过拟合 (Reward Hacking)**: 这是 RLHF 的核心挑战之一。模型可能会学会利用奖励模型的漏洞，生成一些能获得高分但实际质量堪忧的回答。例如，过分追求详细而导致啰嗦、避而不答、或生成“听起来不错”的无意义内容。这要求在数据收集和模型训练中进行仔细的权衡和约束。

-   **评估困境**: 如何准确评估一个模型是否真的与人类价值观“对-齐”是一个开放性问题。现有的基准测试可能无法完全覆盖人类偏好的广度和深度，而 LLM-as-a-Judge 的方法也存在自身的偏见和提示敏感性问题。

-   **多模态对齐**：当前 RLHF 主要集中在文本。如何将其有效扩展到视频、音频等多模态场景，解决**视觉幻觉（Visual Hallucination）、时序理解**等新问题，是一个重要方向。

-   **文化与价值观对齐**：主流的偏好数据大多来自单一文化背景，这使得模型可能无法理解和尊重多样化的文化、价值观和人口特征，甚至会放大偏见。构建更能反映全球多样性的偏好数据集是该领域的一个难题。

-   **效率与成本（低延迟对齐）**：RLHF 流程，特别是基于 PPO 的方法，计算成本高昂。探索更轻量级的对齐方法（例如在**推理时进行对齐 Inference-time Alignment**）和更高效的算法，是降低成本、实现低延迟响应的关键。

针对上述在效率与成本、评估困境及偏见等方面的挑战，学术界和工业界正探索替代或补充人类反馈的路径。其中一个具有代表性的方向是**RLAIF (Reinforcement Learning from AI Feedback)**，它的思路是使用更强大的 AI 模型产生偏好信号，作为人类标注的补充或替代，以降低收集成本并缓解主观偏见。同时，这一方案也可能引入评审模型偏倚与闭环放大等风险，因而需要配套的去偏与校准机制。

## 四、小结

RLHF 是一项开创性的技术，它成功地将复杂的强化学习与大语言模型相结合，为解决模型的“对齐”问题提供了一套行之有效的框架。它通过“SFT 教能力，偏好/RM 学品味，策略优化学对齐”的思想，让大模型更贴近人类期望的智能伙伴。随着 DPO 等新方法的出现和对多模态、多文化等前沿问题的探索，对齐技术正在变得更加高效、鲁棒和公平。

---

## 参考文献

[^1]: [Ouyang, L., Wu, J., et al. (2022). *Training language models to follow instructions with human feedback*. Advances in Neural Information Processing Systems.](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)

[^2]: [Schulman, J., Wolski, F., et al. (2017). *Proximal policy optimization algorithms*. arXiv preprint arXiv:1707.06347.](https://arxiv.org/abs/1707.06347)

[^3]: [Rafailov, R., Zong, A., et al. (2023). *Direct preference optimization: Your language model is secretly a reward model*. arXiv preprint arXiv:2305.18290.](https://arxiv.org/abs/2305.18290)

[^4]: [Cui, B., Lu, Y., et al. (2024). *RLHF: A Comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods*. arXiv preprint arXiv:2401.05583.](https://arxiv.org/abs/2401.05583)
