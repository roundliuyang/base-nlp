# 第二节 词向量表示

在上一章中，简单学习了如何将连续的文本切分成一个个独立的词元（Token）。然而，无论是`jieba`分出的词语，还是BERT模型中的单个汉字，它们本质上仍然是计算机无法直接理解的"字符串"。

机器学习和深度学习模型，无论结构多么复杂，其处理的输入都必须是**数值形式**——具体来说，是由数字组成的**特征向量或矩阵**。因此，在分词之后，必须将这些词元转换为模型可以"消化"的数字形式。这个过程称为**词向量表示 (Word Representation)** 。**词嵌入 (Word Embedding)**通常特指通过神经网络学习得到的稠密向量表示，是词向量表示的一个重要子集。

## 一、为什么需要词向量？

模型无法直接处理文本，它需要数字化的特征。以最基础的NLP任务——**文本分类**为例，需要判断一段文本“国足爱吃海参”属于“负面”类别。模型无法直接理解这串汉字，它只能处理数字。

词向量的核心任务就是**弥合自然语言（符号世界）与数学模型（向量空间）之间的鸿沟**。需要一种系统性的方法，将分词后得到的`["国足", "爱", "吃", "海参"]`这个词元序列，整体转换成一个或一组有意义的数字（即向量），然后才能将其输入给分类模型进行训练和预测。一个好的词向量表示，不仅要能唯一地标识一个词，更理想的情况是，向量本身能够**蕴含词语的语义信息**。例如，我们希望“国王”和“女王”的向量在空间中的距离，会比“国王”和“香蕉”的向量距离更近。实现这一目标，也是NLP领域的重要发展方向之一。

## 二、离散表示

在深度学习普及之前，研究者们提出了多种将词语表示为固定向量的方法。这些方法通常将每个词视为一个独立的、不可再分的单元，生成的向量因此也被称为**离散向量（Discrete Vector）**。其特点是维度高且稀疏，在机器学习领域应用广泛。

### 2.1 独热编码（One-Hot Encoding）

这是最直观、最基础的**词元级别**表示方法。它将每个词元都看作一个独立的类别，其思想与机器学习中的类别特征处理一致。

#### 2.1.1 编码原理

**独热编码**，也称"哑编码"，其核心步骤如下：

1.  **构建词典**：首先，从整个语料库中收集所有出现过的**唯一**词语，构成一个词典。
2.  **分配索引**：为词典中的每个词语分配一个从0开始的唯一整数索引。
3.  **创建向量**：用一个长度等于词典大小的向量来表示每个词。向量中，该词对应索引的位置为`1`，其余所有位置均为`0`。

#### 2.1.2 编码示例

假设词典是从句子"我先挣它一个亿"构建的，分词后为`["我", "先", "挣", "它", "一个", "亿"]`。

-   `我`    -> `[1, 0, 0, 0, 0, 0]`
-   `先`    -> `[0, 1, 0, 0, 0, 0]`
-   `挣`    -> `[0, 0, 1, 0, 0, 0]`
-   `它`    -> `[0, 0, 0, 1, 0, 0]`
-   `一个`  -> `[0, 0, 0, 0, 1, 0]`
-   `亿`    -> `[0, 0, 0, 0, 0, 1]`

#### 2.1.3 优点与缺陷

-   **优点**：实现简单，能够清晰地将词语区分开。
-   **缺点**：
    1.  **维度灾难**：如果词典中有数万个词，那么每个词的向量维度就高达数万，造成数据极其稀疏，浪费计算和存储资源。
    2.  **语义鸿沟**：任意两个不同词的独热向量都是**正交**的（它们的点积为0）。
        
        > **为什么点积为0？**
        > 
        > **点积**是通过将两个向量的对应元素相乘再求和来计算的。在独热编码中，每个向量只有一个位置是`1`，其余都是`0`。对于任意两个**不同**的词，它们值为`1`的位置必然是错开的。
        > 
        > 因此，在计算点积时，`1`总是与`0`相乘，导致所有乘积项都为0，最终的和（点积）也为0。在几何上，点积为0意味着向量**正交**（互相垂直），这在语义上表示所有词都被视为同等的不相似。
        
        意味着模型无法从向量层面得知词与词之间的任何相似关系。在模型看来，"国王"与"女王"的距离，和"国王"与"香蕉"的距离是完全一样的，这严重丢失了语义信息。

### 2.2 词袋模型（Bag-of-Words, BoW）

哑编码表示的是单个词，但实际通常需要表示整个句子或文档。**词袋模型**正是为此而生，它是表示**文档级别**特征最常用的方法之一。这种方法的理论基础可以追溯到向量空间模型 [^1]的提出。

#### 2.2.1 基本思想

**词袋模型**的基本思想是**忽略文本中的词序和语法，将其仅仅视作一个装满词的"袋子"**，用袋子中每个词出现的**统计量**来表示整个文档。

它的实现过程可以理解为：将文档中所有词的**独热向量相加**，得到一个最终的向量。这个向量的维度等于词典大小，每一维的值代表了对应词语在文档中的出现频次。实际实现时，通常直接统计每个词的出现次数，而不需要真正构造和相加 One-Hot 向量。

#### 2.2.2 实现示例

假设词典与上文相同（基于"我先挣它一个亿"），有两个文档：
-   文档1: `我 先 挣 一个 亿`
-   文档2: `我 挣 它 一个 亿`

它们的词袋表示为：
-   `vec(文档1)` = `vec(我)` + `vec(先)` + `vec(挣)` + `vec(一个)` + `vec(亿)` = `[1, 1, 1, 0, 1, 1]`
-   `vec(文档2)` = `vec(我)` + `vec(挣)` + `vec(它)` + `vec(一个)` + `vec(亿)` = `[1, 0, 1, 1, 1, 1]`

这个结果向量的每一维，代表了对应词典中的词在该文档中出现的次数。通过计算两个向量的距离（如余弦相似度），可以发现这两个文档是比较相似的，因为它们共享了"我"、"挣"、"一个"、"亿"这几个词。

#### 2.2.3 余弦相似度计算

**余弦相似度（Cosine Similarity）**通过计算两个向量夹角的余弦值来衡量它们的相似性。对于非负向量（如词袋模型产生的向量），其值范围在 `[0, 1]` 之间，值越接近`1`，表示两个向量越相似。

**公式:**

$$
\text{similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \cdot ||\mathbf{B}||} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \cdot \sqrt{\sum_{i=1}^{n} B_i^2}}
$$

令 **A** = `vec(文档1)` = `[1, 1, 1, 0, 1, 1]`，**B** = `vec(文档2)` = `[1, 0, 1, 1, 1, 1]`

1.  **计算点积 A · B**: $(1 \times 1) + (1 \times 0) + (1 \times 1) + (0 \times 1) + (1 \times 1) + (1 \times 1) = 1 + 0 + 1 + 0 + 1 + 1 = 4$

2.  **计算模长 ||A|| 和 ||B||**:

    `||A||` = $\sqrt{1^2+1^2+1^2+0^2+1^2+1^2} = \sqrt{5}$

    `||B||` = $\sqrt{1^2+0^2+1^2+1^2+1^2+1^2} = \sqrt{5}$

3.  **计算相似度**:

    $$
    \cos(\theta) = \frac{4}{\sqrt{5} \cdot \sqrt{5}} = \frac{4}{5} = 0.8
    $$

结果为`0.8`，这是一个非常接近`1`的值，这从数学上证明了这两个文档是高度相似的。

#### 2.2.4 不同统计方式

词袋模型向量中每一维的值，可以根据不同策略来确定：

- **频数**: 直接使用单词在文档中出现的次数。这是最简单直接的方式，但会受到文章长度影响，长文章的计数值会普遍偏高。
- **频率**: 使用单词在文档中出现的次数除以文档的总词数，即词频（TF）。这在一定程度上缓解了文档长度不同带来的问题。
- **二进制**: 只关心单词是否出现，出现即为`1`，不出现为`0`，不关心出现的次数。

#### 2.2.5 优点与局限

-   **优点**：实现简单，并且在**文本分类**等任务上，因为这类任务的核心在于判断"文档里有什么词"，而非"词与词之间如何关联"，所以即使丢失了词序，也常常能取得不错的效果。
-   **缺点**：
    1.  **丢失词序**：`"我 爱 你"` 和 `"你 爱 我"` 的词袋表示完全相同，无法区分语义差异。
    2.  **未考虑词的重要性**：像"的"、"是"这类在所有文档中都频繁出现的**停用词**，会获得很高的频次，但它们对区分文档主题几乎没有贡献，反而会形成干扰。

### 2.3 N-gram 模型

词袋模型最大的局限在于丢失了词序信息，而 **N-gram（N元语法）** 通过统计连续词组的方式弥补了这一短板。它不仅仅是对传统方法的改进，更是现代大语言模型（LLM）的**鼻祖**，因为它最早引入了**预测下一个词**的思想 [^2]。

#### 2.3.1 预测下一个词

与词袋模型只关心“有什么词”不同，N-gram（1-gram 是特例）关心的是“词的顺序”。其核心基于**马尔可夫假设**，也就是认为**一个词出现的概率只取决于它前面 N-1 个词**。虽然这是一种简化，但它极大地降低了建模的复杂度。

这正是**生成式 AI** 的雏形。根据依赖的前文长度不同，常见的 N-gram 模型包括：
-   **Unigram (1-gram)**: 即词袋模型。假设每个词都是独立的，完全不看前面。
-   **Bigram (2-gram)**: 只看前 **1** 个词。例如看到“喜欢”，预测下一个词是“玩”的概率。
-   **Trigram (3-gram)**: 看前 **2** 个词。例如看到“喜欢 玩”，预测下一个词是“GTA6”的概率。

> 现在的 GPT 等大模型，本质上可以被视为一个 **N 非常非常大**（比如 N=128000）的超级 N-gram 模型。它们都在执行根据上文预测下一个词这一核心任务。

#### 2.3.2 示例

对于句子 `"我 喜欢 玩 GTA6"`：
-   **Bigram 特征**：`{"我 喜欢", "喜欢 玩", "玩 GTA6"}`
-   **Trigram 特征**：`{"我 喜欢 玩", "喜欢 玩 GTA6"}`

通过这种方式，模型就能区分 `"我 喜欢"` 和 `"喜欢 我"` 了，因为它捕捉到了局部的**序列信息**。

#### 2.3.3 维度灾难

虽然 N-gram 找回了词序，但它付出了巨大的代价，这也是它被神经网络取代的原因：

1.  **指数级爆炸**：如果词典有 10,000 个词，Bigram 就有 $10^8$ 种组合，Trigram 有 $10^{12}$ 种... 这种**维度灾难**是传统计算机无法承受的。
2.  **数据稀疏**：绝大多数词的组合（如“大象 驾驶 飞机”）在语料中永远不会出现，导致概率为 0。为了解决这个问题，传统 NLP 发展出了复杂的**平滑技术**，以及通过将词聚类来减少参数的**基于类的 N-gram 模型** [^3]。

正是为了从根本上解决这些问题，后续诞生了**词向量**和**神经网络语言模型**。

### 2.4 TF-IDF

为了提升文档在向量空间中的区分度，解决"常见词权重过高"导致文档混淆的问题，**TF-IDF（Term Frequency-Inverse Document Frequency）** 被提出。正如 Salton 等人所指出的，优秀的索引项应该能降低文档空间的密度，使语义不同的文档在空间中距离更远 [^1]。TF-IDF 就是实现这一目标的经典加权技术。

> TF-IDF 指出：一个词的重要性，与其在**当前文档中出现的次数**成正比，与其在**整个语料库中出现的频率**成反比。一个词在当前文档里越常见，但在其他文档里越罕见，其权重就越高。

#### 2.4.1 计算公式

它由两部分组成：

1.  **词频（Term Frequency, TF）**：衡量一个词在当前文档中出现的频繁程度。常见的计算方式有：

    - 原始频数： $TF(t, d) = f_{t,d}$
    - 归一化频率： $TF(t, d) = \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}}$
    
    其中 $f_{t,d}$ 表示词 $t$ 在文档 $d$ 中出现的次数。

2.  **逆文档频率（Inverse Document Frequency, IDF）**：衡量一个词的"稀有"程度或"信息量"。这一概念由 Karen Sparck Jones 在 1972 年提出 [^4]。

    $$ IDF(t, D) = \log \frac{|D|}{|\{d \in D : t \in d\}|} $$
    
    其中 $|D|$ 是语料库中的总文档数， $|\{d \in D : t \in d\}|$ 是包含词 $t$ 的文档数。
    
    为避免除零错误，实际应用中常使用平滑版本：

    $$ IDF(t, D) = \log \frac{|D|}{1 + |\{d \in D : t \in d\}|} $$

最终, 一个词的 TF-IDF 权重就是这两者的乘积：

$$ TF-IDF(t, d, D) = TF(t, d) \times IDF(t, D) $$

一个文档的 TF-IDF 向量, 就是由该文档中每个词的 TF-IDF 值构成的向量。

#### 2.4.2 实际应用

-   **关键词提取**：计算一篇文章中每个词的 TF-IDF 值，并按降序排列，排在最前面的通常就是这篇文章的关键词。`jieba` 的关键词提取也内置了这种算法。
-   **文本相似度计算**：计算两篇文档的 TF-IDF 向量，再通过余弦相似度等方法判断它们的相似性。
-   **传统搜索引擎**：在早期的搜索引擎中，TF-IDF 是衡量查询词与网页相关性的核心指标之一。

## 三、序号化表示

虽然上述方法在传统机器学习时代扮演了重要角色，但在深度学习时代，它们已不再是主流。现代深度学习模型，尤其是大语言模型，采用的是一种更简洁、更灵活的输入方式——**序号化（Sequentialization）**。

### 3.1 核心思想转变

在深度学习中，通常**只进行最少的预处理**。不再像传统方法那样，费尽心思地设计复杂的特征工程（如计算 TF-IDF）来告诉模型哪些词重要。相反，只把文本转换成最基础的**整数 ID 序列**，然后把"学习词语的含义和重要性"这个更复杂的任务，**交给模型自己去完成**。

### 3.2 序号化过程

**序号化**，也称"整数编码"，是将**分词后的词元序列**转换为深度学习模型能够处理的**整数序列**的**核心步骤**。其过程如下：

1.  **构建词典**：与 One-Hot 类似，首先从训练语料中构建一个词典。但在深度学习中，这个词典通常是**字级别**的（如BERT），或是**子词级别**的（如GPT），而不是词级别的。
2.  **增加特殊词元**：在词典中加入一些有特殊功能的 Token，至少包括：
    -   `[PAD]` (Padding)：用于**填充**。因为模型通常需要批处理（Batch Processing），一个批次内的所有句子必须长度相同。短句子会用`[PAD]`填充到与最长句子一致的长度。其对应的 ID 通常是`0`。
    -   `[UNK]` (Unknown)：用于表示所有在词典中**未出现过**的词。其对应的 ID 通常是`1`。
    -   此外，还可能有 `[CLS]` (Classification), `[SEP]` (Separator) 等用于特定任务的特殊词元。
3.  **ID 映射**：将文本序列中的每个词元（字/子词）直接映射为其在词典中的**整数 ID**。

**预训练模型的词典：**

在实践中，很少从零开始为自己的小数据集构建词典。更常见的做法是，直接使用像 `BERT`、`GPT` 这类预训练模型官方提供的 **词典文件（vocab.txt）** 。这些词典通常包含了数万个字、子词、符号等，是在海量通用语料上构建的，覆盖面非常广。

例如，Google 的中文 BERT 模型词典 `vocab.txt` 中就包含了约 21128 个词元，其中不仅有常用汉字，还包括了英文字母、数字、标点及 `[PAD]`, `[UNK]` 等特殊符号。

### 3.3 序号化实例

假设有一个精简词典：

`{'[PAD]': 0, '[UNK]': 1, '比': 2, '方': 3, '说': 4, '我': 5, '先': 6, '挣': 7, '它': 8, '一': 9, '个': 10, '亿': 11}`

现在有三个句子需要处理：
1.  `我挣一个亿`
2.  `比方说我`
3.  `我先挣钱`

**第一步：分词 & 查找ID**
-   句子1 (`我挣一个亿`): `我` (5), `挣` (7), `一` (9), `个` (10), `亿` (11) -> `[5, 7, 9, 10, 11]`
-   句子2 (`比方说我`): `比` (2), `方` (3), `说` (4), `我` (5) -> `[2, 3, 4, 5]`
-   句子3 (`我先挣钱`): `我` (5), `先` (6), `挣` (7), `钱` (不在词典中) -> `[5, 6, 7, 1]`

**第二步：填充 (Padding)**
为了将这三个长短不一的序列组成一个矩阵，需要以最长的序列（句子1，长度为5）为基准，对其他短序列用`[PAD]`的ID `0`进行填充。

-   序列1 (长度5): `[5, 7, 9, 10, 11]`
-   序列2 (长度4→5): `[2, 3, 4, 5, 0]`
-   序列3 (长度4→5): `[5, 6, 7, 1, 0]`

最终，我们得到一个 `3x5` 的**整数矩阵**。这个矩阵，就是喂给深度学习模型的最终输入。

```bash
# 最终输入模型的张量（Tensor）
[[5, 7, 9, 10, 11],
 [2, 3, 4, 5,  0],
 [5, 6, 7, 1,  0]]
```

> 序号化本身并未解决语义鸿沟，其整数ID（如 `2` 和 `3`）不具备数学意义。它的真正价值是作为后续**嵌入层**的输入。嵌入层会将这些ID查询并映射为低维、稠密的浮点数向量（即**词向量**），而这个映射关系本身是在模型训练中**学习**出来的 [^5]。

---

## 参考文献

[^1]: [Salton, G., Wong, A., & Yang, C. S. (1975). *A vector space model for automatic indexing*. Communications of the ACM, 18(11), 613-620](https://doi.org/10.1145/361219.361220)

[^2]: [Shannon, C. E. (1951). *Prediction and entropy of printed English*. Bell System Technical Journal, 30(1), 50-64](https://doi.org/10.1002/j.1538-7305.1951.tb01366.x)

[^3]: [Brown, P. F., Desouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). *Class-based n-gram models of natural language*. Computational Linguistics, 18(4), 467-479](https://aclanthology.org/J92-4003/)

[^4]: [Jones, K. S. (1972). *A statistical interpretation of term specificity and its application in retrieval*. Journal of Documentation, 28(1), 11-21](https://doi.org/10.1108/eb026526)

[^5]: [Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). *A neural probabilistic language model*. Journal of Machine Learning Research, 3(Feb), 1137-1155](https://www.jmlr.org/papers/v3/bengio03a.html)

