# ç¬¬ä¸€èŠ‚ æ‰‹æ“ä¸€ä¸ªå¤§æ¨¡å‹

å‰é¢æˆ‘ä»¬å·²ç»æ·±å…¥å­¦ä¹ äº† **æ³¨æ„åŠ›æœºåˆ¶**ã€**Transformer æ¶æ„**ï¼Œä»¥åŠåŸºäºå…¶ Encoder è¡ç”Ÿçš„ **BERT** å’ŒåŸºäº Decoder è¡ç”Ÿçš„ **GPT**ã€‚æ¥ä¸‹æ¥å°è¯•äº²æ‰‹å®ç°ä¸€ä¸ªï¼ˆæ›¾ç»çš„ï¼‰å‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼Œçœ‹çœ‹å®ƒçš„æ¨¡å‹ç»“æ„ç©¶ç«Ÿæœ‰ä»€ä¹ˆä¸åŒã€‚

æœ¬èŠ‚å°†èšç„¦äº Llama2ï¼Œä¸€ä¸ªç”± Meta AI æ¨å‡ºçš„å¼€æºå¤§æ¨¡å‹ã€‚æˆ‘ä»¬ä¸å†ä¾èµ– `transformers` åº“çš„é«˜åº¦å°è£…ï¼Œè€Œæ˜¯ä»é›¶å¼€å§‹ï¼Œå…ˆæ¢³ç†å…³é”®æ€æƒ³ä¸è®¾è®¡å–èˆï¼Œå†é€æ­¥è½åœ°åˆ°ä»£ç å®ç°ã€‚è¿™ä¸€è¿‡ç¨‹å°†å¸®åŠ©ä½ ç†è§£åŸç†ï¼Œæ·±åŒ–å¯¹å¤§æ¨¡å‹å†…éƒ¨å·¥ä½œçš„ç†è§£ã€‚

> [æœ¬èŠ‚å®Œæ•´ä»£ç ](https://github.com/datawhalechina/base-nlp/tree/main/code/C6/llama2)

## ä¸€ã€Llama2 æ¶æ„æ€»è§ˆ

Llama2 éµå¾ªäº† GPT ç³»åˆ—å¼€åˆ›çš„ **Decoder-Only** æ¶æ„ã€‚è¿™æ„å‘³ç€å®ƒå®Œå…¨ç”± **Transformer è§£ç å™¨å±‚**å †å è€Œæˆï¼Œå¤©ç„¶é€‚ç”¨äºè‡ªå›å½’çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚

å¦‚å›¾ 6-1 æ‰€ç¤ºï¼ŒLlama2 çš„æ ¸å¿ƒç”± N ä¸ªç›¸åŒçš„ Transformer Block å †å è€Œæˆã€‚Block å†…éƒ¨çš„æ•°æ®æµå±•ç¤ºäº† Llama2 çš„è®¾è®¡ï¼š

- **é¢„å½’ä¸€åŒ–ï¼ˆPre-Normalizationï¼‰**ï¼šä¸ç»å…¸ Transformer çš„åå½’ä¸€åŒ–ä¸åŒï¼Œè¾“å…¥åœ¨è¿›å…¥æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆç½‘ç»œ**ä¹‹å‰**ï¼Œéƒ½ä¼šå…ˆç»è¿‡ä¸€æ¬¡ `RMS Norm`ã€‚è¿™è¢«è®¤ä¸ºæ˜¯æå‡å¤§æ¨¡å‹è®­ç»ƒç¨³å®šæ€§çš„å…³é”®ï¼ˆæˆ‘ä»¬æ›¾æåˆ°è¿‡ï¼ŒGPT-2/3 æ­£æ˜¯è½¬å‘ Pre-Norm è§£å†³äº†æ·±å±‚ç½‘ç»œçš„è®­ç»ƒéš¾é¢˜ï¼‰ã€‚
- **ç»„ä»¶å‡çº§**ï¼šæ”¯æŒ `Grouped-Query Attentionï¼ˆGQAï¼‰`ï¼ˆå¦‚ Llama2-70B é‡‡ç”¨ [^1]ï¼›å°æ¨¡å‹å¯è§†ä¸º `n_kv_heads == n_heads` çš„ MHA ç‰¹ä¾‹ï¼‰ï¼Œå‰é¦ˆç½‘ç»œé‡‡ç”¨ `SwiGLU`ï¼Œå½’ä¸€åŒ–ä½¿ç”¨ `RMSNorm`ã€‚
- **æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰**ï¼šå›¾ä¸­å¯è§ï¼Œä½ç½®ä¿¡æ¯å¹¶éåœ¨è¾“å…¥ç«¯ä¸è¯åµŒå…¥ç›¸åŠ ï¼Œè€Œæ˜¯åœ¨æ³¨æ„åŠ›å±‚å†…éƒ¨ï¼Œé€šè¿‡ `RoPE` æ“ä½œåŠ¨æ€åœ°æ–½åŠ äºæŸ¥è¯¢ï¼ˆQï¼‰å’Œé”®ï¼ˆKï¼‰å‘é‡ä¹‹ä¸Šã€‚
- **æ®‹å·®è¿æ¥**ï¼šæ¯ä¸ªå­å±‚ï¼ˆæ³¨æ„åŠ›å±‚å’Œå‰é¦ˆç½‘ç»œï¼‰çš„è¾“å‡ºéƒ½é€šè¿‡æ®‹å·®è¿æ¥ï¼ˆ`+`å·ï¼‰ä¸å­å±‚çš„è¾“å…¥ç›¸åŠ ï¼Œä¿ç•™äº†åŸå§‹ä¿¡æ¯æµã€‚

æ•´ä¸ªæ¨¡å‹çš„æ•°æ®æµè‡ªä¸‹è€Œä¸Šè´¯ç©¿æ‰€æœ‰ Transformer Blockï¼Œæœ€åç»è¿‡ä¸€æ¬¡æœ€ç»ˆçš„ `RMS Norm` å’Œä¸€ä¸ªçº¿æ€§è¾“å‡ºå±‚ï¼Œå¾—åˆ° Logitsã€‚

<p align="center">
  <img src="./images/6_1_1.svg" width="40%" alt="Llama2 æ¶æ„å›¾" />
  <br />
  <em>å›¾ 6-1ï¼šLlama2 æ¶æ„ç¤ºæ„å›¾</em>
</p>

ä¸åŸå§‹ Transformer è§£ç å™¨ç›¸æ¯”ï¼ŒLlama2 åŠå…¶åŒç±»æ¨¡å‹è¿›è¡Œäº†ä¸€ç³»åˆ—æ”¹è¿›ï¼Œä»¥æå‡æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚å®ƒçš„æ•°æ®æµå¯ä»¥æ¦‚æ‹¬ä¸ºï¼š

1.  **è¾“å…¥åµŒå…¥**ï¼šå°† `token_ids` è½¬æ¢ä¸ºè¯å‘é‡ã€‚
2.  **N x Transformer å±‚å †å **ï¼šæ•°æ®ä¾æ¬¡é€šè¿‡ N ä¸ªç›¸åŒçš„ Transformer Blockã€‚
    -   **é¢„å½’ä¸€åŒ–**ï¼šåœ¨è¿›å…¥å­å±‚ä¹‹å‰ï¼Œå…ˆè¿›è¡Œä¸€æ¬¡ RMSNormã€‚
    -   **æ³¨æ„åŠ›å­ç³»ç»Ÿ**ï¼šåŒ…å«**æ—‹è½¬ä½ç½®ç¼–ç **ã€**åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰** å’Œ **KV ç¼“å­˜**æœºåˆ¶ã€‚
    -   **å‰é¦ˆç½‘ç»œå­ç³»ç»Ÿ**ï¼šé‡‡ç”¨ **SwiGLU** æ¿€æ´»å‡½æ•°ã€‚
3.  **æœ€ç»ˆå½’ä¸€åŒ–ä¸è¾“å‡º**ï¼šåœ¨æ‰€æœ‰å±‚ä¹‹åï¼Œè¿›è¡Œæœ€åä¸€æ¬¡ RMSNormï¼Œå¹¶é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚å°†ç‰¹å¾æ˜ å°„åˆ°è¯æ±‡è¡¨ logitsã€‚

ä¸‹é¢ï¼Œæˆ‘ä»¬å°†æ ¹æ®å›¾ 6-1 ä¸­ Llama2 çš„ç»“æ„é¡ºåºï¼Œä»è¾“å…¥ç«¯å¼€å§‹ï¼Œé€ä¸€å®ç°å…¶æ ¸å¿ƒç»„ä»¶ã€‚

## äºŒã€å…³é”®ç»„ä»¶è¯¦è§£

### 2.1 é¢„å½’ä¸€åŒ–

#### 2.1.1 è®¾è®¡æ€è·¯

æ ‡å‡†çš„ Layer Normalization åœ¨ Transformer ä¸­ç”¨äºç¨³å®šè®­ç»ƒï¼Œä½†å®ƒçš„è®¡ç®—ï¼ˆå‡å»å‡å€¼ã€é™¤ä»¥æ ‡å‡†å·®ï¼‰ç›¸å¯¹å¤æ‚ã€‚ä¸ºäº†åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æå‡è®¡ç®—æ•ˆç‡ï¼ŒLlama2 é‡‡ç”¨äº†å®ƒçš„å˜ä½“ **RMSNormï¼ˆRoot Mean Square Layer Normalizationï¼‰** [^2]ã€‚

å…¶ç›®çš„æ˜¯ **ç®€åŒ–å½’ä¸€åŒ–è¿‡ç¨‹**ï¼š
- **ç§»é™¤å‡å€¼ä¸­å¿ƒåŒ–**ï¼šåªé€šè¿‡è¾“å…¥çš„å‡æ–¹æ ¹ï¼ˆRoot Mean Squareï¼‰å¯¹å®ƒè¿›è¡Œç¼©æ”¾ã€‚
- **ä¿ç•™å¯å­¦ä¹ å¢ç›Š**ï¼šä¾ç„¶ä¿ç•™ä¸€ä¸ªå¯å­¦ä¹ çš„ `weight` å‚æ•° ($\gamma$)ï¼Œç”¨äºåœ¨å½’ä¸€åŒ–åæ¢å¤æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

å…¬å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ $x$ æ˜¯è¾“å…¥å‘é‡ï¼Œ $\gamma$ æ˜¯å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼š

$$
y = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2 + \epsilon}} \cdot \gamma
$$

#### 2.1.2 æ¥å£å®šä¹‰

- **è¾“å…¥**: ä¸€ä¸ªå½¢çŠ¶ä¸º `[batch_size, seq_len, dim]` çš„å¼ é‡ `x`ã€‚
- **è¾“å‡º**: ä¸€ä¸ªä¸è¾“å…¥å½¢çŠ¶ç›¸åŒçš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªè¯å…ƒ (`dim` ç»´åº¦) éƒ½è¢«ç‹¬ç«‹å½’ä¸€åŒ–ã€‚

> ä¹‹å‰çš„å­¦ä¹ ä¸­æˆ‘ä»¬å·²ç»çŸ¥é“ï¼ŒåŸå§‹çš„æ–‡æœ¬æ•°æ®é¦–å…ˆä¼šè¢«åˆ†è¯å™¨ï¼ˆTokenizerï¼‰è½¬æ¢æˆä¸€ä¸ªç”±æ•´æ•°IDç»„æˆçš„åºåˆ—ã€‚ä¸ºäº†è¿›è¡Œæ‰¹å¤„ç†ï¼Œæˆ‘ä»¬ä¼šå°†å¤šä¸ªè¿™æ ·çš„åºåˆ—æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªå½¢çŠ¶ä¸º `[batch_size, seq_len]` çš„äºŒç»´å¼ é‡ã€‚éšåï¼Œè¿™ä¸ªå¼ é‡ä¼šç»è¿‡ä¸€ä¸ªè¯åµŒå…¥å±‚ï¼ˆEmbedding Layerï¼‰ï¼Œå°†æ¯ä¸ªæ•´æ•°IDæ˜ å°„æˆä¸€ä¸ªé«˜ç»´å‘é‡ã€‚è¿™ä¸ªå‘é‡çš„ç»´åº¦å°±æ˜¯ `dim`ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ª `[batch_size, seq_len, dim]` å½¢çŠ¶çš„ä¸‰ç»´å¼ é‡ï¼Œè¿™å°±æ˜¯ Transformer Block çš„æ ‡å‡†è¾“å…¥ã€‚

#### 2.1.3 ä»£ç å®ç° (`src/norm.py`)

```python
# code/C6/llama2/src/norm.py
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim)) # å¯¹åº”å…¬å¼ä¸­çš„ gamma

    def _norm(self, x: torch.Tensor) -> torch.Tensor:
        # æ ¸å¿ƒè®¡ç®—ï¼šx * (x^2çš„å‡å€¼ + eps)çš„å¹³æ–¹æ ¹çš„å€’æ•°
        return x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = self._norm(x.float()).type_as(x)
        return out * self.weight
```

-   `_norm` æ–¹æ³•ç²¾ç¡®åœ°å®ç°äº† RMSNorm çš„æ ¸å¿ƒå…¬å¼ã€‚
-   `self.eps` æ˜¯ä¸€ä¸ªä¸ºäº†é˜²æ­¢é™¤ä»¥é›¶è€Œæ·»åŠ çš„å°å¸¸æ•°ï¼Œä¿è¯äº†æ•°å€¼ç¨³å®šæ€§ã€‚

#### 2.1.4 å•å…ƒæµ‹è¯•

ä¸ºäº†ç¡®ä¿æ¯ä¸ªæ¨¡å—çš„ç‹¬ç«‹å¯ç”¨æ€§å’Œæ­£ç¡®æ€§ï¼Œæˆ‘ä»¬ä¸ºå…¶æ·»åŠ ä¸€ä¸ª `if __name__ == "__main__"` æµ‹è¯•å—ã€‚è¿™æ˜¯ä¸€ç§è‰¯å¥½çš„å·¥ç¨‹å®è·µï¼Œå¯ä»¥å•ç‹¬è¿è¡Œæ­¤æ–‡ä»¶æ¥å¿«é€ŸéªŒè¯åŠŸèƒ½ã€‚

```python
# code/C6/llama2/src/norm.py
if __name__ == "__main__":
    # å‡†å¤‡å‚æ•°å’Œè¾“å…¥
    batch_size, seq_len, dim = 4, 16, 64
    x = torch.randn(batch_size, seq_len, dim)

    # åˆå§‹åŒ–å¹¶åº”ç”¨ RMSNorm
    norm = RMSNorm(dim)
    output = norm(x)

    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    print("--- RMSNorm Test ---")
    print("Input shape:", x.shape)
    print("Output shape:", output.shape)
```

### 2.2 æ—‹è½¬ä½ç½®ç¼–ç 

#### 2.2.1 è®¾è®¡æ€è·¯

æˆ‘ä»¬åœ¨ Transformer ç« èŠ‚ä¸­å·²ç»çŸ¥é“ï¼Œæ¨¡å‹éœ€è¦ä½ç½®ä¿¡æ¯æ¥ç†è§£è¯å…ƒçš„é¡ºåºã€‚ä¼ ç»Ÿçš„ä½ç½®ç¼–ç ï¼ˆæ— è®ºæ˜¯å›ºå®šçš„è¿˜æ˜¯å¯å­¦ä¹ çš„ï¼‰æ˜¯ä¸€ç§ç»å¯¹ä½ç½®ç¼–ç ï¼Œå®ƒä¸ºæ¯ä¸ªä½ç½®åˆ†é…ä¸€ä¸ªç‹¬ç«‹çš„å‘é‡ã€‚

Llama2 åˆ™é‡‡ç”¨äº†æ›´å…ˆè¿›çš„ **æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRotary Positional Embedding, RoPEï¼‰** [^3]ï¼Œå®ƒæ˜¯ä¸€ç§**ç›¸å¯¹ä½ç½®ç¼–ç **ã€‚ä¸ä¼ ç»Ÿä½ç½®ç¼–ç é€šè¿‡**åŠ æ³•**ç›´æ¥æ³¨å…¥è¯åµŒå…¥çš„æ–¹å¼ä¸åŒï¼ŒRoPE çš„ç­–ç•¥æ˜¯ï¼š**ä½ç½®ä¿¡æ¯ä¸å†æ˜¯"åŠ "åˆ°è¯åµŒå…¥ä¸Šï¼Œè€Œæ˜¯åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œé€šè¿‡å¤æ•°"ä¹˜æ³•"çš„æ–¹å¼"æ—‹è½¬"Query å’Œ Key å‘é‡**ã€‚

<p align="center">
  <img src="./images/6_1_2.png" width="80%" alt="RoPE æ—‹è½¬ä½ç½®ç¼–ç ç¤ºæ„å›¾" />
  <br />
  <em>å›¾ 6-2ï¼šRoPE æ—‹è½¬ä½ç½®ç¼–ç çš„å·¥ä½œåŸç†</em>
</p>

å¦‚å›¾ 6-2 æ‰€ç¤ºï¼ŒRoPE é€šè¿‡å¤æ•°ä¹˜æ³•å®ç°å‘é‡æ—‹è½¬ï¼š

- **æ•°å­¦åŸç†**ï¼šå°†å‘é‡çš„æ¯å¯¹ç»´åº¦ $(x_1, x_2)$ è§†ä¸ºå¤æ•° $x_1 + ix_2$ã€‚å¤æ•°å¯è¡¨ç¤ºä¸º $r e^{i\theta}$ï¼ˆ$r$ ä¸ºæ¨¡ï¼Œ$\theta$ ä¸ºå¹…è§’ï¼‰ï¼Œä¸¤å¤æ•°ç›¸ä¹˜æ—¶æ¨¡ç›¸ä¹˜ã€å¹…è§’ç›¸åŠ ï¼š$(r_1 e^{i\theta_1}) \cdot (r_2 e^{i\theta_2}) = r_1 r_2 e^{i(\theta_1 + \theta_2)}$ã€‚RoPE çš„ `freqs_cis` æ˜¯æ¨¡ä¸º 1 çš„å¤æ•° $e^{im\theta}$ï¼ˆ$m$ ä¸ºä½ç½®ï¼‰ï¼Œä¸ Q/K å‘é‡ç›¸ä¹˜åå¾—åˆ°æ—‹è½¬åçš„ $(x'_1, x'_2)$ï¼Œåªæ”¹å˜æ–¹å‘è€Œä¸æ”¹å˜é•¿åº¦ã€‚
- **ä½ç½®ç¼–ç **ï¼šåºåˆ—ä¸­æ¯ä¸ªä½ç½®ï¼ˆ1-6ï¼‰çš„ Query/Key å‘é‡è¢«æ—‹è½¬ä¸åŒçš„è§’åº¦ï¼ˆä½ç½®è¶Šé åæ—‹è½¬è¶Šå¤§ï¼Œé¢œè‰²å˜åŒ–ä½“ç°äº†è¿™ä¸€ç‚¹ï¼‰ï¼Œä»è€Œå°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°å‘é‡çš„æ–¹å‘ä¸Šã€‚

**RoPE çš„ä¼˜åŠ¿**ï¼š
- **ç›¸å¯¹ä½ç½®ç¼–ç **ï¼šä¸¤ä¸ªè¯å…ƒï¼ˆä½ç½® $m$ å’Œ $n$ï¼‰æ—‹è½¬åçš„ Q/K ç‚¹ç§¯ä»…ä¸ç›¸å¯¹è·ç¦» $m-n$ ç›¸å…³ï¼Œä¸ç»å¯¹ä½ç½®æ— å…³ã€‚è¿™ä½¿å¾—æ³¨æ„åŠ›æ¨¡å¼å…·å¤‡å¹³ç§»ä¸å˜æ€§â€”â€”ç›¸è· 2 ä¸ªä½ç½®çš„è¯å…ƒå…³ç³»ï¼Œæ— è®ºå‡ºç°åœ¨åºåˆ—ä½•å¤„ï¼Œè®¡ç®—æ–¹å¼éƒ½ä¸€è‡´ã€‚
- **é•¿åº¦å¤–æ¨èƒ½åŠ›**ï¼šç”±äºä¾èµ–ç›¸å¯¹ä½ç½®ï¼Œæ¨¡å‹å¯¹è¶…å‡ºè®­ç»ƒé•¿åº¦çš„åºåˆ—ä»èƒ½è¾ƒå¥½åœ°å¤„ç†ä½ç½®å…³ç³»ã€‚
- **è®¡ç®—é«˜æ•ˆ**ï¼šé€šè¿‡å¤æ•°ä¹˜æ³•å®ç°ï¼Œä¸æ”¹å˜å‘é‡æ¨¡é•¿ï¼Œé¿å…äº†é¢å¤–çš„å½’ä¸€åŒ–æ“ä½œã€‚

#### 2.2.2 æ¥å£å®šä¹‰

RoPE çš„å®ç°åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

1.  **`precompute_freqs_cis`**:
    -   **åŠŸèƒ½**: é¢„è®¡ç®—ä¸€ä¸ªåŒ…å«æ—‹è½¬è§’åº¦ä¿¡æ¯çš„å¤æ•°å¼ é‡ `freqs_cis`ã€‚è¿™ä¸ªå¼ é‡åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶è®¡ç®—ä¸€æ¬¡å³å¯ã€‚
    -   **è¾“å…¥**:
        -   `dim`: head çš„ç»´åº¦ã€‚
        -   `end`: åºåˆ—æœ€å¤§é•¿åº¦ã€‚
        -   `theta`: ä¸€ä¸ªç”¨äºæ§åˆ¶é¢‘ç‡èŒƒå›´çš„è¶…å‚æ•°ã€‚
    -   **è¾“å‡º**: å½¢çŠ¶ä¸º `[end, dim / 2]` çš„å¤æ•°å¼ é‡ã€‚

2.  **`apply_rotary_emb`**:
    -   **åŠŸèƒ½**: å°†é¢„è®¡ç®—çš„ `freqs_cis` åº”ç”¨äºè¾“å…¥çš„ Query å’Œ Key å‘é‡ã€‚
    -   **è¾“å…¥**:
        -   `xq`: Query å‘é‡ï¼Œå½¢çŠ¶ `[batch_size, seq_len, n_heads, head_dim]`ã€‚
        -   `xk`: Key å‘é‡ï¼Œå½¢çŠ¶ `[batch_size, seq_len, n_kv_heads, head_dim]`ã€‚
        -   `freqs_cis`: é¢„è®¡ç®—çš„æ—‹è½¬çŸ©é˜µåˆ‡ç‰‡ã€‚
    -   **è¾“å‡º**: æ—‹è½¬åçš„ `xq` å’Œ `xk`ï¼Œå½¢çŠ¶ä¸å˜ã€‚

> æˆ‘ä»¬çŸ¥é“ï¼Œè¿›å…¥æ³¨æ„åŠ›æ¨¡å—çš„å¼ é‡ `x` çš„å½¢çŠ¶æ˜¯ `[batch_size, seq_len, dim]`ã€‚ä¸ºäº†å®ç°å¤šå¤´æ³¨æ„åŠ›ï¼Œé¦–å…ˆè¦å°†è¿™ä¸ªå¼ é‡é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆä¾‹å¦‚ `wq`ï¼‰ï¼Œå®ƒå°†è¾“å…¥ä» `dim` ç»´æŠ•å½±åˆ° `n_heads * head_dim` ç»´ã€‚åœ¨ Llama2 çš„è®¾è®¡ä¸­ï¼Œè¾“å…¥ç»´åº¦ `dim` æ°å¥½ç­‰äº `n_heads * head_dim`ï¼Œæ‰€ä»¥è¿™ä¸ªçº¿æ€§å±‚å®é™…ä¸Šæ˜¯ä¸€ä¸ª `dim` åˆ° `dim` çš„æŠ•å½±ï¼Œå…¶è¾“å‡ºå¼ é‡å½¢çŠ¶ä¾ç„¶æ˜¯ `[batch_size, seq_len, dim]`ã€‚å…³é”®çš„ä¸€æ­¥å‘ç”Ÿåœ¨ä¹‹åï¼šæˆ‘ä»¬åˆ©ç”¨ `dim = n_heads * head_dim` è¿™ä¸€å…³ç³»ï¼Œé€šè¿‡ä¸€æ¬¡ `view` æˆ– `reshape` æ“ä½œï¼Œå°†æœ€åä¸€ä¸ªç»´åº¦ `dim` é€»è¾‘ä¸Šæ‹†åˆ†ä¸º `n_heads` å’Œ `head_dim` ä¸¤ä¸ªç»´åº¦ï¼Œä»è€Œå¾—åˆ° `[batch_size, seq_len, n_heads, head_dim]` è¿™æ ·çš„å››ç»´å¼ é‡ã€‚è¿™ä¸ªå½¢çŠ¶çš„å«ä¹‰æ˜¯ï¼šå¯¹äºæ¯ä¸ªè¯å…ƒï¼Œæˆ‘ä»¬éƒ½è®¡ç®—å‡ºäº† `n_heads` ä¸ªç‹¬ç«‹çš„ã€ç»´åº¦ä¸º `head_dim` çš„ Query å‘é‡è¡¨ç¤ºã€‚å¯¹ Key å‘é‡ `xk` çš„å¤„ç†ä¹Ÿæ˜¯å®Œå…¨ç±»ä¼¼çš„ã€‚

#### 2.2.3 ä»£ç å®ç° (`src/rope.py`)

**1. `precompute_freqs_cis`**:

```python
def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:
    # 1. è®¡ç®—é¢‘ç‡ï¼š1 / (theta^(2i/dim))
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # 2. ç”Ÿæˆä½ç½®åºåˆ— t = [0, 1, ..., end-1]
    t = torch.arange(end, device=freqs.device)
    # 3. è®¡ç®—ç›¸ä½ï¼št å’Œ freqs çš„å¤–ç§¯
    freqs = torch.outer(t, freqs).float()
    # 4. è½¬æ¢ä¸ºå¤æ•°å½¢å¼ (cos(theta) + i*sin(theta))
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis
```

å…¶ä¸­ `torch.arange(0, dim, 2) / dim` å¯¹åº”å…¬å¼ä¸­çš„ `2i/dim`ï¼š`i` å®é™…éå†çš„æ˜¯å¶æ•°ç»´ç´¢å¼•ï¼ˆé•¿åº¦ä¸º `dim/2`ï¼‰ã€‚

**2. `reshape_for_broadcast`**: è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå°† `freqs_cis` çš„å½¢çŠ¶è°ƒæ•´ä¸ºå¯ä»¥ä¸ Q/K å‘é‡è¿›è¡Œå¹¿æ’­ä¹˜æ³•ã€‚

```python
def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    ndim = x.ndim
    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)
```

**3. `apply_rotary_emb`**:

```python
def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
) -> tuple[torch.Tensor, torch.Tensor]:
    # å°† Q/K å‘é‡è§†ä¸ºå¤æ•°
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    
    # å‡†å¤‡å¹¿æ’­
    freqs_q = reshape_for_broadcast(freqs_cis, xq_)  # é’ˆå¯¹ Q çš„å¹¿æ’­è§†å›¾
    
    # å¤æ•°ä¹˜æ³•å³ä¸ºæ—‹è½¬
    xq_out = torch.view_as_real(xq_ * freqs_q).flatten(3)
    
    # K å‘é‡å¯èƒ½ä¸ Q å‘é‡æœ‰ä¸åŒçš„å¤´æ•°ï¼ˆGQAï¼‰ï¼Œæ‰€ä»¥éœ€å•ç‹¬ç”Ÿæˆå¹¿æ’­è§†å›¾
    freqs_k = reshape_for_broadcast(freqs_cis, xk_)
    xk_out = torch.view_as_real(xk_ * freqs_k).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xq)
```
-   `torch.view_as_complex` å°† `head_dim` ç»´çš„å®æ•°å‘é‡å·§å¦™åœ°çœ‹ä½œ `head_dim/2` ç»´çš„å¤æ•°å‘é‡ã€‚
-   æ ¸å¿ƒæ“ä½œ `xq_ * freqs_cis` **æ­£æ˜¯æ—‹è½¬çš„å®ç°**ã€‚åœ¨å¤æ•°åŸŸä¸­ï¼Œä¸¤ä¸ªå¤æ•°ç›¸ä¹˜å³è¡¨ç¤ºå¹…è§’ç›¸åŠ ã€æ¨¡ç›¸ä¹˜ã€‚ç”±äº `freqs_cis` çš„æ¨¡ä¸º1ï¼Œè¿™ä¸ªæ“ä½œå°±ç­‰ä»·äºå°† `xq_` å‘é‡æ—‹è½¬ `freqs_cis` æ‰€ä»£è¡¨çš„è§’åº¦ã€‚
-   é€šè¿‡åˆ†åˆ«ä¸º Q å’Œ K ç”Ÿæˆå¹¿æ’­è§†å›¾ï¼ˆ`freqs_q` ä¸ `freqs_k`ï¼‰æ¥å…¼å®¹ GQA å¸¦æ¥çš„å½¢çŠ¶å·®å¼‚ã€‚
-   **å‚æ•° `theta`**: RoPE çš„â€œåŸºåº•â€ï¼Œæ§åˆ¶ä½ç½®ç¼–ç çš„é¢‘ç‡èŒƒå›´ï¼Œ`10000.0` æ˜¯ä¸€ä¸ªæ ‡å‡†å€¼ã€‚
-   **å·¥ç¨‹è€ƒé‡**: åœ¨ `LlamaTransformer` åˆå§‹åŒ–æ—¶ï¼Œé¢„è®¡ç®—çš„é•¿åº¦é€šå¸¸ä¼šå¤§äº `max_seq_len`ï¼ˆä¾‹å¦‚ `max_seq_len * 2`ï¼‰ï¼Œä¸ºæ¨ç†æ—¶å¤„ç†æ›´é•¿åºåˆ—æä¾›â€œç¼“å†²â€ï¼Œé¿å…é‡æ–°è®¡ç®—ã€‚

#### 2.2.4 å•å…ƒæµ‹è¯•

`rope.py` æ–‡ä»¶åŒ…å«äº†ä¸‰ä¸ªæ ¸å¿ƒå‡½æ•°ã€‚ç¤ºä¾‹æµ‹è¯•ä»…æ‰“å°è¾“å…¥/è¾“å‡ºå½¢çŠ¶ä»¥ä¾¿å¿«é€Ÿæ ¸å¯¹ï¼Œä¸ä½¿ç”¨æ–­è¨€ã€‚

```python
# code/C6/llama2/src/rope.py
if __name__ == "__main__":
    # å‡†å¤‡å‚æ•°å’Œè¾“å…¥
    batch_size, seq_len, n_heads, n_kv_heads, head_dim = 4, 16, 8, 2, 16
    dim = n_heads * head_dim
    n_rep = n_heads // n_kv_heads

    # --- Test precompute_freqs_cis ---
    print("--- Test precompute_freqs_cis ---")
    freqs_cis = precompute_freqs_cis(dim=head_dim, end=seq_len * 2)
    print("freqs_cis shape:", freqs_cis.shape)

    # --- Test apply_rotary_emb ---
    print("\n--- Test apply_rotary_emb ---")
    xq = torch.randn(batch_size, seq_len, n_heads, head_dim)
    xk = torch.randn(batch_size, seq_len, n_kv_heads, head_dim)
    freqs_cis_slice = freqs_cis[:seq_len]
    xq_out, xk_out = apply_rotary_emb(xq, xk, freqs_cis_slice)
    print("xq shape (in/out):", xq.shape, xq_out.shape)
    print("xk shape (in/out):", xk.shape, xk_out.shape)
```

### 2.3 åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)

#### 2.3.1 è®¾è®¡æ€è·¯

æ ‡å‡†çš„**å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attention, MHAï¼‰** ä¸ºæ¯ä¸ª Query å¤´éƒ½é…å¤‡äº†ä¸€ç»„ç‹¬ç«‹çš„ Key å’Œ Value å¤´ã€‚è¿™æ„å‘³ç€ K å’Œ V æŠ•å½±çŸ©é˜µçš„å°ºå¯¸ä»¥åŠæ¨ç†æ—¶ KV ç¼“å­˜çš„å¤§å°éƒ½ä¸æ€»å¤´æ•° `n_heads` æˆæ­£æ¯”ï¼Œå½“æ¨¡å‹è§„æ¨¡å¢å¤§æ—¶ï¼Œè¿™éƒ¨åˆ†å¼€é”€å˜å¾—éå¸¸æ˜¾è‘—ã€‚

**åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGrouped-Query Attention, GQAï¼‰**[^4] æ˜¯å¯¹æ­¤çš„æ ¸å¿ƒä¼˜åŒ–ã€‚å…¶æ€è·¯æ˜¯ï¼š**å…è®¸å¤šä¸ª Query å¤´å…±äº«åŒä¸€ç»„ Key å’Œ Value å¤´**ã€‚

- **MHA**: æ¯ä¸ª Q å¤´éƒ½æœ‰è‡ªå·±çš„ K/V å¤´ï¼ˆ`n_heads` == `n_kv_heads`ï¼‰ã€‚
- **GQA**: æ¯ç»„ Q å¤´å…±äº«ä¸€ç»„ K/V å¤´ï¼ˆ`n_heads` > `n_kv_heads`ï¼‰ã€‚
- **MQA**: æ‰€æœ‰ Q å¤´å…±äº«å”¯ä¸€çš„ä¸€ç»„ K/V å¤´ï¼ˆ`n_kv_heads` = 1ï¼‰ï¼Œæ˜¯ GQA çš„ç‰¹ä¾‹ã€‚

é€šè¿‡åˆ†ç»„ï¼ŒGQA åœ¨ä¿æŒ MHA å¤§éƒ¨åˆ†æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº† K/V ç›¸å…³çš„è®¡ç®—é‡å’Œæ˜¾å­˜å ç”¨ï¼Œå¯¹åŠ é€Ÿæ¨¡å‹æ¨ç†è‡³å…³é‡è¦ã€‚

**è¿™å¸¦æ¥äº†ä»€ä¹ˆå¥½å¤„ï¼Ÿ**
- **æ˜¾å­˜èŠ‚çœ**ï¼šKV ç¼“å­˜çš„å¤§å°ç›´æ¥ä» `n_heads` ç›¸å…³é™ä½åˆ° `n_kv_heads` ç›¸å…³ï¼Œå‡å°‘ä¸ºåŸæ¥çš„ `n_kv_heads / n_heads`ã€‚å¯¹äº 70B æ¨¡å‹ï¼Œè¿™èƒ½èŠ‚çœæ•°å GB çš„æ˜¾å­˜ã€‚
- **è®¡ç®—åŠ é€Ÿ**ï¼šæ³¨æ„åŠ›è®¡ç®—ä¸­çš„ K/V æŠ•å½±å’Œåç»­çš„çŸ©é˜µä¹˜æ³•è®¡ç®—é‡ä¹Ÿç›¸åº”å‡å°‘ã€‚

**ä¸¾ä¸ªæ —å­ğŸŒ°**ï¼šå‡è®¾ä¸€ä¸ªæ¨¡å‹æœ‰ `n_heads = 32` ä¸ªæŸ¥è¯¢å¤´ï¼Œå¦‚æœä½¿ç”¨ GQA å¹¶è®¾ç½® `n_kv_heads = 8`ï¼Œé‚£ä¹ˆ Key å’Œ Value ç›¸å…³çš„å‚æ•°é‡ã€è®¡ç®—é‡ä»¥åŠ KV ç¼“å­˜å¤§å°éƒ½å°†**å‡å°‘åˆ°åŸæ¥çš„ 1/4**ã€‚

#### 2.3.2 æ¥å£å®šä¹‰

- **è¾“å…¥**:
    - `x`: å½¢çŠ¶ä¸º `[batch_size, seq_len, dim]` çš„å¼ é‡ã€‚
    - `start_pos`, `freqs_cis`, `mask`: ä¸æ ‡å‡† Attention ç±»ä¼¼ï¼Œç”¨äº KV ç¼“å­˜ã€ä½ç½®ç¼–ç å’Œå› æœé®è”½ã€‚
- **è¾“å‡º**: å½¢çŠ¶ä¸º `[batch_size, seq_len, dim]` çš„å¼ é‡ã€‚
- **å…³é”®å®ç°**: åœ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°å‰ï¼Œéœ€è¦å°† K å’Œ V çš„å¤´â€œå¤åˆ¶â€ `n_rep` æ¬¡ï¼ˆ`n_rep = n_heads / n_kv_heads`ï¼‰ï¼Œä½¿å…¶æ•°é‡ä¸ Q å¤´åŒ¹é…ï¼Œä»¥ä¾¿è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚

#### 2.3.3 ä»£ç å®ç° (`src/attention.py`)

ä¸ºå®ç° GQA çš„å¤´æ•°å¯¹é½ï¼Œéœ€è¦è¾…åŠ©å‡½æ•° `repeat_kv`ï¼ˆå®šä¹‰åœ¨ `rope.py`ï¼‰ã€‚ä¹‹æ‰€ä»¥æ”¾åœ¨ `rope.py`ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬å°†ä¸æ³¨æ„åŠ›è®¡ç®—ç›¸å…³çš„â€œæ— çŠ¶æ€å¼ é‡ç®—å­â€ï¼ˆå¦‚ RoPE çš„ `apply_rotary_emb`ã€`precompute_freqs_cis` ä»¥åŠå¤´å¤åˆ¶ `repeat_kv`ï¼‰é›†ä¸­åˆ°åŒä¸€å¤„ï¼Œä¾¿äºå¤ç”¨ã€è§£è€¦ `attention.py` çš„ç±»å®ç°ï¼Œå¹¶é¿å…å¼•å…¥æ›´é‡çš„ä¾èµ–ã€‚è¯¥å‡½æ•°é€šè¿‡ `expand` å’Œ `reshape` å°† `[batch_size, seq_len, n_kv_heads, head_dim]` çš„ K/V å¼ é‡æŒ‰ `n_rep` å¤åˆ¶ä¸º `[batch_size, seq_len, n_kv_heads * n_rep, head_dim]`ï¼Œä»¥ä¸ Q å¤´æ•°å¯¹é½ã€‚

```python
# code/C6/llama2/src/rope.py
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    batch_size, seq_len, n_kv_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (
        x[:, :, :, None, :]
        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)
        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)
    )
```

æ¥ä¸‹æ¥ï¼Œå®ç° `GroupedQueryAttention` ç±»ã€‚

```python
# code/C6/llama2/src/attention.py
class GroupedQueryAttention(nn.Module):
    def __init__(self, dim: int, n_heads: int, n_kv_heads: int | None = None, ...):
        ...
        self.n_local_heads = n_heads
        self.n_local_kv_heads = n_kv_heads
        self.n_rep = self.n_local_heads // self.n_local_kv_heads # Qå¤´ä¸KVå¤´çš„é‡å¤æ¯”
        ...
        self.wq = nn.Linear(dim, n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)
        ...

    def forward(self, x, start_pos, freqs_cis, mask):
        xq = self.wq(x).view(batch_size, seq_len, self.n_local_heads, self.head_dim)
        xk = self.wk(x).view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = self.wv(x).view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)

        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
        
        # ... KV Cache é€»è¾‘ ...

        keys = repeat_kv(keys, self.n_rep)   # <-- å…³é”®æ­¥éª¤
        values = repeat_kv(values, self.n_rep) # <-- å…³é”®æ­¥éª¤

        scores = torch.matmul(xq.transpose(1, 2), keys.transpose(1, 2).transpose(2, 3)) / ...
        ...
```

-   `wq`, `wk`, `wv` çš„è¾“å‡ºç»´åº¦ä¸åŒï¼Œåˆ†åˆ«å¯¹åº” `n_heads` å’Œ `n_kv_heads`ï¼Œç›´æ¥ä½“ç°äº† GQA çš„è®¾è®¡ã€‚
-   åœ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ä¹‹å‰ï¼Œé€šè¿‡ `repeat_kv` å‡½æ•°å°† K å’Œ V çš„å¤´è¿›è¡Œæ‰©å±•ï¼Œä½¿å…¶æ•°é‡ä¸ Q å¤´åŒ¹é…ï¼Œä»è€Œèƒ½å¤Ÿè¿›è¡Œæ ‡å‡†çš„æ³¨æ„åŠ›è®¡ç®—ã€‚

#### 2.3.4 å•å…ƒæµ‹è¯•

GQA æ¨¡å—çš„æµ‹è¯•éœ€è¦å®Œæ•´åˆå§‹åŒ– `GroupedQueryAttention` ç±»ï¼Œå¹¶ä¸ºå…¶ `forward` æ–¹æ³•å‡†å¤‡å¥½æ‰€æœ‰å¿…éœ€çš„è¾“å…¥ï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿçš„ `freqs_cis`ã€‚æµ‹è¯•çš„æ ¸å¿ƒæ˜¯éªŒè¯ç»è¿‡æ•´ä¸ªæ³¨æ„åŠ›è®¡ç®—æµç¨‹åï¼Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶æ˜¯å¦ä¸è¾“å…¥ä¸€è‡´ã€‚

```python
# code/C6/llama2/src/attention.py
if __name__ == "__main__":
    # å‡†å¤‡å‚æ•°å’Œè¾“å…¥
    batch_size, seq_len, dim = 4, 16, 128
    n_heads, n_kv_heads = 8, 2
    head_dim = dim // n_heads

    # åˆå§‹åŒ–æ³¨æ„åŠ›æ¨¡å—
    attention = GroupedQueryAttention(
        dim=dim,
        n_heads=n_heads,
        n_kv_heads=n_kv_heads,
        max_batch_size=batch_size,
        max_seq_len=seq_len,
    )

    # å‡†å¤‡è¾“å…¥
    x = torch.randn(batch_size, seq_len, dim)
    freqs_cis = precompute_freqs_cis(dim=head_dim, end=seq_len * 2)
    freqs_cis_slice = freqs_cis[:seq_len]

    # æ‰§è¡Œå‰å‘ä¼ æ’­
    output = attention(x, start_pos=0, freqs_cis=freqs_cis_slice)

    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    print("--- GroupedQueryAttention Test ---")
    print("Input shape:", x.shape)
    print("Output shape:", output.shape)
```

### 2.4 SwiGLU å‰é¦ˆç½‘ç»œ

#### 2.4.1 è®¾è®¡æ€è·¯

Transformer ä¸­çš„å‰é¦ˆç½‘ç»œä¸ºæ¨¡å‹æä¾›äº†éçº¿æ€§è®¡ç®—èƒ½åŠ›ï¼Œé€šå¸¸ç”±ä¸¤ä¸ªçº¿æ€§å±‚å’Œä¸€ä¸ª ReLU æ¿€æ´»å‡½æ•°æ„æˆã€‚Llama2 é‡‡ç”¨äº†ä¸€ç§å˜ä½“ **SwiGLU**[^5]ï¼Œå®ƒè¢«è¯æ˜èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚

å®ƒçš„æ ¸å¿ƒæ˜¯å¼•å…¥**é—¨æ§æœºåˆ¶**ï¼š
- å®ƒä½¿ç”¨ä¸‰ä¸ªçº¿æ€§å˜æ¢ï¼ˆ`W`, `V`, `W2`ï¼‰è€Œä¸æ˜¯ä¸¤ä¸ªã€‚
- ç¬¬ä¸€ä¸ªå˜æ¢ `xW` ç»è¿‡ Swish æ¿€æ´»å‡½æ•°ï¼ˆ`swish(x) = x * sigmoid(x)`ï¼‰ã€‚
- ç¬¬äºŒä¸ªå˜æ¢ `xV` ä½œä¸ºâ€œé—¨â€ï¼Œä¸å‰ä¸€æ­¥çš„ç»“æœè¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚
- æœ€åé€šè¿‡ç¬¬ä¸‰ä¸ªå˜æ¢ `W2` è¾“å‡ºã€‚

å…¬å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ $\otimes$ æ˜¯é€å…ƒç´ ä¹˜æ³•ï¼š
$$
\text{SwiGLU}(x, W, V, W_2) = (\text{swish}(xW) \otimes xV)W_2
$$

è¿™ç§é—¨æ§ç»“æ„å…è®¸ç½‘ç»œåŠ¨æ€åœ°æ§åˆ¶ä¿¡æ¯æµï¼Œè¢«è®¤ä¸ºæ˜¯å…¶æ€§èƒ½ä¼˜äºæ ‡å‡† ReLU FFN çš„åŸå› ã€‚

#### 2.4.2 æ¥å£å®šä¹‰

- **è¾“å…¥**: `x`ï¼Œå½¢çŠ¶ä¸º `[batch_size, seq_len, dim]` çš„å¼ é‡ã€‚
- **è¾“å‡º**: å½¢çŠ¶ä¸è¾“å…¥ç›¸åŒçš„å¼ é‡ `[batch_size, seq_len, dim]`ã€‚
- **å†…éƒ¨ç»´åº¦**: ä¸­é—´éšè—å±‚çš„ç»´åº¦ `hidden_dim` é€šå¸¸ä¼šå¤§äº `dim`ï¼ŒLlama2 ä¸­é€šè¿‡ç‰¹å®šå…¬å¼è®¡ç®—å¹¶å¯¹é½ï¼Œä»¥æé«˜ç¡¬ä»¶è®¡ç®—æ•ˆç‡ã€‚

#### 2.4.3 ä»£ç å®ç° (`src/ffn.py`)

```python
# code/C6/llama2/src/ffn.py
class FeedForward(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, ...):
        super().__init__()
        # hidden_dim è®¡ç®—ï¼Œå¹¶ç”¨ multiple_of å¯¹é½ä»¥æé«˜ç¡¬ä»¶æ•ˆç‡
        hidden_dim = int(2 * hidden_dim / 3)
        ...
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        self.w1 = nn.Linear(dim, hidden_dim, bias=False) # å¯¹åº” W
        self.w2 = nn.Linear(hidden_dim, dim, bias=False) # å¯¹åº” W2
        self.w3 = nn.Linear(dim, hidden_dim, bias=False) # å¯¹åº” V

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # F.silu(self.w1(x)) å®ç°äº† swish(xW)
        # * self.w3(x) å®ç°äº†é—¨æ§æœºåˆ¶
        return self.w2(torch.nn.functional.silu(self.w1(x)) * self.w3(x))
```

- `torch.nn.functional.silu` å°±æ˜¯ PyTorch å†…ç½®çš„ Swish æ¿€æ´»å‡½æ•°ã€‚
- æ•´ä¸ª `forward` å‡½æ•°å‡†ç¡®åœ°å®ç°äº† SwiGLU çš„å…¬å¼ã€‚

#### 2.4.4 å•å…ƒæµ‹è¯•

æœ€åï¼Œä¸º `FeedForward` æ¨¡å—æ·»åŠ æµ‹è¯•ä»£ç ï¼ŒéªŒè¯å…¶èƒ½å¦æ­£ç¡®å¤„ç†è¾“å…¥å¼ é‡å¹¶è¿”å›ç›¸åŒå½¢çŠ¶çš„è¾“å‡ºã€‚

```python
# code/C6/llama2/src/ffn.py
if __name__ == "__main__":
    # å‡†å¤‡å‚æ•°å’Œè¾“å…¥
    batch_size, seq_len, dim = 4, 16, 128
    
    # åˆå§‹åŒ– FFN æ¨¡å—
    ffn = FeedForward(
        dim=dim,
        hidden_dim=4 * dim,
        multiple_of=256,
        ffn_dim_multiplier=None
    )

    # å‡†å¤‡è¾“å…¥
    x = torch.randn(batch_size, seq_len, dim)

    # æ‰§è¡Œå‰å‘ä¼ æ’­
    output = ffn(x)

    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    print("--- FeedForward (SwiGLU) Test ---")
    print("Input shape:", x.shape)
    print("Output shape:", output.shape)
```

## ä¸‰ã€æ¨¡å‹ç»„è£…ä¸å‰å‘ä¼ æ’­

æœ‰äº†æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬ç»„è£…æˆä¸€ä¸ªå®Œæ•´çš„ `LlamaTransformer` äº†ã€‚

**ä»£ç å®ç°** (`src/transformer.py`):

1.  **`TransformerBlock`**: è¿™æ˜¯æ„æˆ Llama2 çš„åŸºæœ¬å•å…ƒã€‚

    ```python
    # code/C6/llama2/src/transformer.py
    class TransformerBlock(nn.Module):
        def __init__(self, layer_id: int, ...):
            ...
            self.attention = GroupedQueryAttention(...)
            self.feed_forward = FeedForward(...)
            self.attention_norm = RMSNorm(...)
            self.ffn_norm = RMSNorm(...)

        def forward(self, x, start_pos, freqs_cis, mask):
            # é¢„å½’ä¸€åŒ– + æ®‹å·®è¿æ¥
            h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)
            out = h + self.feed_forward(self.ffn_norm(h))
            return out
    ```
    -   å®ƒæ¸…æ™°åœ°å±•ç¤ºäº† **é¢„å½’ä¸€åŒ–** ç»“æ„ï¼šå…ˆ `RMSNorm`ï¼Œå†é€å…¥ `attention` æˆ– `feed_forward`ï¼Œæœ€åè¿›è¡Œæ®‹å·®è¿æ¥ã€‚

2.  **`LlamaTransformer`**: é¡¶å±‚æ¨¡å‹ï¼Œè´Ÿè´£å †å  `TransformerBlock` å¹¶å¤„ç†è¾“å…¥è¾“å‡ºã€‚

    ```python
    # code/C6/llama2/src/transformer.py
    class LlamaTransformer(nn.Module):
        def __init__(self, vocab_size: int, ...):
            ...
            self.tok_embeddings = nn.Embedding(vocab_size, dim)
            self.layers = nn.ModuleList([TransformerBlock(...) for i in range(n_layers)])
            self.norm = RMSNorm(dim, eps=norm_eps)
            self.output = nn.Linear(dim, vocab_size, bias=False)
            self.register_buffer("freqs_cis", precompute_freqs_cis(...))

        def forward(self, tokens: torch.Tensor, start_pos: int) -> torch.Tensor:
            h = self.tok_embeddings(tokens)
            
            # 1. å‡†å¤‡ RoPE æ—‹è½¬çŸ©é˜µ
            freqs_cis = self.freqs_cis[start_pos : start_pos + seq_len]

            # 2. å‡†å¤‡å› æœæ©ç  (Causal Mask)
            mask = None
            if seq_len > 1:
                mask = torch.full((seq_len, seq_len), float("-inf"), device=tokens.device)
                mask = torch.triu(mask, diagonal=1)
                # è€ƒè™‘ KV Cache çš„åç§»
                mask = torch.hstack([torch.zeros((seq_len, start_pos), ...), mask]).type_as(h)

            # 3. å¾ªç¯é€šè¿‡æ‰€æœ‰ TransformerBlock
            for layer in self.layers:
                h = layer(h, start_pos, freqs_cis, mask)
            
            h = self.norm(h)
            logits = self.output(h).float()
            return logits
    ```
    -   `tok_embeddings`: å°† token ID è½¬æ¢ä¸ºå‘é‡ã€‚
    -   `layers`: ä½¿ç”¨ `nn.ModuleList` å †å  N ä¸ª `TransformerBlock`ã€‚
    -   `norm` å’Œ `output`: æœ€ç»ˆçš„å½’ä¸€åŒ–å’Œçº¿æ€§è¾“å‡ºå±‚ã€‚
    -   `freqs_cis`: é¢„å…ˆè®¡ç®—å¹¶ç¼“å­˜ RoPE æ—‹è½¬çŸ©é˜µã€‚
    -   **`forward` æµç¨‹**:
        1.  `freqs_cis` åˆ‡ç‰‡ï¼šæ ¹æ®å½“å‰è¾“å…¥çš„ `start_pos` å’Œ `seq_len`ï¼Œä»é¢„è®¡ç®—çš„æ—‹è½¬çŸ©é˜µä¸­å–å‡ºéœ€è¦çš„éƒ¨åˆ†ã€‚
        2.  `mask` æ„é€ ï¼šè¿™æ˜¯å®ç° **å› æœè¯­è¨€æ¨¡å‹** çš„å…³é”®ã€‚`torch.triu` åˆ›å»ºäº†ä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œç¡®ä¿æ¯ä¸ªä½ç½®åªèƒ½å…³æ³¨åˆ°å®ƒè‡ªå·±å’Œå®ƒä¹‹å‰çš„ä½ç½®ã€‚`torch.hstack` åˆ™è€ƒè™‘äº† `start_pos`ï¼Œè¿™æ˜¯ä¸ºäº†é…åˆ **KV ç¼“å­˜**ï¼ˆåœ¨æ¨ç†æ—¶ `start_pos > 0`ï¼‰ï¼Œç¡®ä¿å½“å‰ Query å¯ä»¥å…³æ³¨åˆ°ç¼“å­˜ä¸­æ‰€æœ‰çš„å†å² Keyã€‚
        3.  å¾ªç¯è°ƒç”¨ `TransformerBlock`ï¼Œé€å±‚å¤„ç†ç‰¹å¾ã€‚
        4.  æœ€ç»ˆé€šè¿‡ `norm` å’Œ `output` å±‚å¾—åˆ° logitsã€‚

## å››ã€æ•´ä½“éªŒè¯

### 4.1 å¿«é€ŸéªŒè¯

åœ¨æ‰€æœ‰ç»„ä»¶å®ç°å¹¶ç»„è£…åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•è„šæœ¬æ¥éªŒè¯æ•´ä¸ª `LlamaTransformer` æ¨¡å‹çš„è¾“å…¥è¾“å‡ºæ˜¯å¦ç¬¦åˆé¢„æœŸã€‚

```python
# code/C6/llama2/main.py
import torch
from src.transformer import LlamaTransformer

def main() -> None:
    # ä½¿ç”¨å°å°ºå¯¸å‚æ•°ï¼Œä¾¿äº CPU/GPU éƒ½èƒ½å¿«é€Ÿè·‘é€š
    model = LlamaTransformer(
        vocab_size=1000,
        dim=256,
        n_layers=2,
        n_heads=8,
        n_kv_heads=2,
        multiple_of=64,
        ffn_dim_multiplier=None,
        norm_eps=1e-6,
        max_batch_size=4,
        max_seq_len=64,
    )

    # æ„é€ éšæœº token åºåˆ—å¹¶æ‰§è¡Œå‰å‘
    batch_size, seq_len = 2, 16
    tokens = torch.randint(0, 1000, (batch_size, seq_len))
    logits = model(tokens, start_pos=0)

    # æœŸæœ›: [batch_size, seq_len, vocab_size]
    print("logits shape:", tuple(logits.shape))

if __name__ == "__main__":
    main()
```

ä½ å°†ä¼šçœ‹åˆ°å¦‚ä¸‹è¾“å‡ºï¼Œè¿™è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹å·²ç»èƒ½å¤Ÿæ­£ç¡®å¤„ç†è¾“å…¥å¹¶è¿”å›ç¬¦åˆé¢„æœŸçš„ logits å¼ é‡ï¼š

```text
logits shape: (2, 16, 1000)
```

è¿™ä¸ªè„šæœ¬å®ä¾‹åŒ–äº†ä¸€ä¸ªå°å‹çš„ `LlamaTransformer`ï¼Œå¹¶ç”¨ä¸€ä¸ªéšæœºçš„ `tokens` å¼ é‡ï¼ˆä»£è¡¨ä¸€ä¸ªæ‰¹æ¬¡ã€é•¿åº¦ä¸º16çš„ä¸¤ä¸ªå¥å­ï¼‰ä½œä¸ºè¾“å…¥ï¼Œæ‰§è¡Œäº†æ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œæœ€ç»ˆéªŒè¯äº†è¾“å‡º `logits` çš„å½¢çŠ¶ä¸ `[batch_size, seq_len, vocab_size]` åŒ¹é…ã€‚

---

## å‚è€ƒæ–‡çŒ®

[^1]: [Touvron, H., Martin, L., Stone, K., et al. (2023). *Llama 2: Open Foundation and Fine-Tuned Chat Models*.](https://arxiv.org/abs/2307.09288)

[^2]: [Zhang, J., & Sennrich, R. (2019). *Root Mean Square Layer Normalization*. NeurIPS 2019.](https://arxiv.org/abs/1910.07467)

[^3]: [Su, J., Lu, Y., Pan, S., et al. (2021). *RoFormer: Enhanced Transformer with Rotary Position Embedding*.](https://arxiv.org/abs/2104.09864)

[^4]: [Ainslie, J., Dossel, J., Ontanon, S., et al. (2023). *GQA: Training Generalized Multi-Query Attention Models from Multi-Head Checkpoints*.](https://arxiv.org/abs/2305.13245)

[^5]: [Shazeer, N. (2020). *GLU Variants Improve Transformer*.](https://arxiv.org/abs/2002.05202)
